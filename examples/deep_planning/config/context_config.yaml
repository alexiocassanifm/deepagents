# ================================================================================
# DEEP PLANNING AGENT - SIMPLIFIED CONTEXT MANAGEMENT
# ================================================================================
# 
# Simplified configuration focused ONLY on token counting and compression triggers.
# Uses LiteLLM for accurate token counting across all models (Claude, GPT, LLaMA, etc.)
#
# 🎯 WHAT IT CONTROLS:
# - When automatic context compression triggers based on token count
# - Token counting thresholds for different scenarios
#
# 🔧 HOW TO MODIFY:
# - Values 0.0-1.0 are percentages of max_context_window
# - Lower thresholds = more aggressive/frequent compression
# - Higher thresholds = more conservative/rare compression
#
# ================================================================================

# =============================================================================
# 🧠 TOKEN-BASED COMPRESSION TRIGGERS
# =============================================================================
context_management:
  # 📏 Maximum context window size (in tokens)
  # When this size is reached, compression becomes mandatory
  # Typical: 200K for Claude models, 128K for GPT-4, 50K for testing
  max_context_window: 50000
  
  # 🎯 Main threshold for automatic compaction (0.0-1.0)
  # When context reaches this % of max window, compression triggers
  # Example: 0.25 = compress when reaching 25% of 50K = 12.5K tokens
  trigger_threshold: 0.25
  
  # 🔧 POST_TOOL Hook - Trigger after every tool call (0.0-1.0)
  # The hook checks context after every tool call and compresses if it exceeds this threshold
  # THIS IS THE MOST IMPORTANT THRESHOLD for real-time automatic compression
  # Example: 0.20 = automatically compress when context > 20% after tool call (10K tokens)
  post_tool_threshold: 0.20
  
  # 📦 Enable automatic compaction when thresholds reached
  # If false, system monitors but never compresses automatically
  auto_compaction: true
  
  # 📝 Enable detailed logging of compression operations
  # Useful for debug and monitoring token usage
  logging_enabled: true

# =============================================================================
# 📦 COMPACTION CONFIGURATION
# =============================================================================
compaction:
  # 📝 Template for summary generation during compression
  summary_template: "Context Summary - Deep Planning Session"
  
  # 💬 Always preserve the last user message
  # Important to maintain context of current user request
  preserve_last_user_message: true
  
  # 🔄 Continuation format for seamless agent resumption after compression
  continuation_format: "deep_planning_standard"

# =============================================================================
# ⚡ PERFORMANCE OPTIMIZATION
# =============================================================================
performance:
  # 💾 Cache duration for context analysis results (seconds)
  # Avoids re-analyzing same content repeatedly
  analysis_cache_duration: 60
  
  # 🎯 Use LiteLLM for accurate token counting across all models
  # Supports Claude, GPT, LLaMA, and all models supported by LiteLLM
  use_litellm_tokenization: true
  
  # 🔄 Fallback token estimation if LiteLLM unavailable
  # Uses characters/4 approximation as backup
  fallback_token_estimation: true

# =============================================================================
# 📊 MONITORING
# =============================================================================
monitoring:
  # 📈 Collect metrics on compression operations
  collect_metrics: true
  
  # 📝 Log level for compression operations
  log_level: "INFO"
  
  # 📤 Export compression statistics  
  export_statistics: true
  export_format: "json"

# ================================================================================
# 🤖 LLM-BASED COMPRESSION (Enhanced for Software Design)
# ================================================================================
llm_compression:
  # 🎯 Enable LLM-based semantic compression for better accuracy
  enabled: true
  
  # 🧠 Model configuration for compression tasks
  # Using Haiku for fast, cost-effective compression
  model_name: "claude-3-haiku-20240307"
  
  # 🎛️ Compression type optimized for software design workflows
  # Options: general, mcp_heavy, code_focused, planning_focused, technical_deep
  compression_type: "planning_focused"
  
  # 📏 Compression strategy and targets
  strategy: "balanced"  # 50-60% reduction vs aggressive (70-80%) or conservative (30-40%)
  target_reduction_percentage: 60.0
  max_output_tokens: 3000
  
  # ⏱️ Performance and reliability settings
  timeout_seconds: 10
  fallback_to_pattern: true  # Use pattern-matching if LLM fails
  enable_caching: true  # Cache results to avoid duplicate LLM calls
  cache_duration_seconds: 300  # 5 minutes
  
  # 🎯 Software Design Focus - Elements to preserve
  preserve_elements:
    - user_stories
    - architectural_guidelines
    - technical_requirements
    - non_functional_requirements
    - dependencies
    - design_decisions
    - risk_areas
    - integration_points
    - security_requirements
    - performance_requirements
  
  # 🔍 Analysis categories for software design
  analysis_categories:
    primary_requests: "Main user goals and business requirements"
    technical_concepts: "Technologies, frameworks, patterns, and architecture decisions"
    architectural_requirements: "Non-functional requirements (security, scalability, performance)"
    files_and_code: "Code references, modules, and implementation details"
    dependencies: "Cross-component dependencies and integration requirements"
    design_decisions: "Architecture choices and technical trade-offs made"
    risk_areas: "Potential technical debt and complexity hotspots"
    current_work: "Active implementation focus and progress"
    next_steps: "Pending tasks and immediate priorities"
  
  # 📊 Quality and monitoring
  min_success_rate: 0.95  # If success rate drops below this, disable LLM compression
  max_fallback_rate: 0.10  # Alert if fallback rate exceeds this threshold

# ================================================================================
# 📋 CONFIGURATION NOTES
# ================================================================================
# 
# This simplified configuration focuses on core functionality:
# - Accurate token counting with LiteLLM (works with all models)
# - Simple threshold-based compression triggering
# - Essential logging and monitoring
# 
# 🔧 TO ADJUST COMPRESSION BEHAVIOR:
# - More aggressive: Lower trigger_threshold (0.15-0.20) and post_tool_threshold (0.15-0.18)
# - More conservative: Raise trigger_threshold (0.30-0.40) and post_tool_threshold (0.25-0.30)
#
# 🚀 BENEFITS OF SIMPLIFIED APPROACH:
# - Universal token counting for all models
# - No false positives from pattern matching
# - Clean, maintainable configuration
# - Focus on what matters: token limits and compression
#
# ================================================================================